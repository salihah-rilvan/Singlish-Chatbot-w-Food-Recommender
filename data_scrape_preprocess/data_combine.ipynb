{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# ../hardwarezone_clean/\n",
    "# ../reddit_singapore_cleaned/\n",
    "data_dir_cleaned = \"../data/hardwarezone_clean/\"\n",
    "for f in sorted(os.listdir(data_dir_cleaned)):\n",
    "    path_f = data_dir_cleaned + f\n",
    "    clean_text_list = open(path_f, 'r').readlines()\n",
    "    contexted = []\n",
    "\n",
    "    n = 7\n",
    "\n",
    "    for i in range(n, len(clean_text_list)):\n",
    "        row = []\n",
    "        prev = i - 1 - n # we additionally substract 1, so row will contain current response and 7 previous responses  \n",
    "        for j in range(i, prev, -1):\n",
    "            row.append(clean_text_list[j])\n",
    "        contexted.append(row)\n",
    "    columns = ['response', 'context'] \n",
    "    columns = columns + ['context/'+str(i) for i in range(n-1)]\n",
    "\n",
    "    df = pd.DataFrame.from_records(contexted, columns=columns)\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_cleaned = \"../data/reddit_singapore_cleaned/\"\n",
    "for f in sorted(os.listdir(data_dir_cleaned)):\n",
    "    path_f = data_dir_cleaned + f\n",
    "    clean_text_list = open(path_f, 'r').readlines()\n",
    "    contexted = []\n",
    "\n",
    "    n = 7\n",
    "\n",
    "    for i in range(n, len(clean_text_list)):\n",
    "        row = []\n",
    "        prev = i - 1 - n # we additionally substract 1, so row will contain current response and 7 previous responses  \n",
    "        for j in range(i, prev, -1):\n",
    "            row.append(clean_text_list[j])\n",
    "        contexted.append(row)\n",
    "    columns = ['response', 'context'] \n",
    "    columns = columns + ['context/'+str(i) for i in range(n-1)]\n",
    "\n",
    "    df = pd.DataFrame.from_records(contexted, columns=columns)\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat(dfs).reset_index(drop=True)\n",
    "all_df = all_df.sample(50000).reset_index(drop=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, validate_df = train_test_split(all_df, random_state=42, test_size=0.2)\n",
    "\n",
    "train_df.to_csv('../data/train_scrape_df.csv', index=False)\n",
    "validate_df.to_csv('../data/validate_scrape_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('../data/train_df.csv')\n",
    "df2 = pd.read_csv('../data/train_scrape_df.csv')\n",
    "df3 = pd.read_csv('../data/validate_df.csv')\n",
    "df4 = pd.read_csv('../data/validate_scrape_df.csv')\n",
    "\n",
    "df = pd.concat([df1, df2, df3, df4]).reset_index(drop=True)\n",
    "\n",
    "train_df, validate_df = train_test_split(df, random_state=42, test_size=0.2)\n",
    "\n",
    "train_df.to_csv('../data/train_combine_df.csv', index=False)\n",
    "validate_df.to_csv('../data/validate_combine_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
